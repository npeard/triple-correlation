{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import numpy as np\n",
    "from biphase_gpt.nano_gpt import GPT, GPTConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fluo import Fluorescence1D, Fluorescence2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = int(1e4)\n",
    "num_pix = 21\n",
    "Phi_dim = num_pix // 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Model Memory Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_1D_dataset_size():\n",
    "    print('\\n1D dataset and model sizes')\n",
    "    float_size = 32\n",
    "    target_size = num_pix  # full phase is 2*num_pix-1\n",
    "    input_size = (Phi_dim - 1) ** 2\n",
    "\n",
    "    inputs_data_size = input_size * float_size * num_samples\n",
    "    targets_data_size = target_size * float_size * num_samples\n",
    "\n",
    "    config = GPTConfig(\n",
    "        in_seq_len=input_size, out_seq_len=target_size, n_layer=2, n_head=4, n_embd=64\n",
    "    )\n",
    "    print('Model size:', GPT(config).get_num_params() / 1e6, 'M parameters\\r\\n')\n",
    "\n",
    "    print(\n",
    "        f'Inputs for {num_pix} pixels with {num_samples} samples consume',\n",
    "        inputs_data_size / 1e9,\n",
    "        'GB\\r\\n',\n",
    "    )\n",
    "    print(\n",
    "        f'Targets for {num_pix} pixels with {num_samples} samples consume',\n",
    "        targets_data_size / 1e9,\n",
    "        'GB\\r\\n',\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Inputs for {num_pix} pixels with {num_samples} samples consume',\n",
    "        inputs_data_size / 1e9,\n",
    "        'GB',\n",
    "    )\n",
    "    print(\n",
    "        f'Targets for {num_pix} pixels with {num_samples} samples consume',\n",
    "        targets_data_size / 1e9,\n",
    "        'GB',\n",
    "    )\n",
    "\n",
    "\n",
    "def print_2D_dataset_size(print_model_size=True):\n",
    "    print('\\n2D dataset and model sizes')\n",
    "    float_size = 32\n",
    "    target_size = (\n",
    "        num_pix\n",
    "    ) ** 2  # full phase is (2*num_pix-1)**2, we only need one quadrant\n",
    "    input_size = (Phi_dim - 1) ** 4\n",
    "    # We will only give inputs to one quadrant at a time\n",
    "    # Inference will be run on at least two quadrants\n",
    "\n",
    "    inputs_data_size = input_size * float_size * num_samples\n",
    "    targets_data_size = target_size * float_size * num_samples\n",
    "\n",
    "    if print_model_size:\n",
    "        config = GPTConfig(\n",
    "            in_seq_len=input_size,\n",
    "            out_seq_len=target_size,\n",
    "            n_layer=2,\n",
    "            n_head=4,\n",
    "            n_embd=64,\n",
    "        )\n",
    "        print('Model size:', GPT(config).get_num_params() / 1e6, 'M parameters\\r\\n')\n",
    "\n",
    "    print(\n",
    "        f'Inputs for {num_pix} pixels with {num_samples} samples consume',\n",
    "        inputs_data_size / 1e9,\n",
    "        'GB',\n",
    "    )\n",
    "    print(\n",
    "        f'Targets for {num_pix} pixels with {num_samples} samples consume',\n",
    "        targets_data_size / 1e9,\n",
    "        'GB',\n",
    "    )\n",
    "\n",
    "\n",
    "print_1D_dataset_size()\n",
    "print_2D_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing 1D Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nGenerating {num_samples} samples...')\n",
    "Phi_samples = np.zeros((num_samples, Phi_dim, Phi_dim))\n",
    "phase_samples = np.zeros((num_samples, num_pix))\n",
    "for i in tqdm(range(num_samples)):\n",
    "    # Generate random phase\n",
    "    phase = np.random.uniform(-np.pi, np.pi, num_pix)\n",
    "    phase[0] = 0\n",
    "    phase_samples[i, :] = phase\n",
    "\n",
    "    # Compute Phi matrix\n",
    "    Phi = Fluorescence1D.compute_Phi_from_phase(phase)\n",
    "    Phi_samples[i, :, :] = Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean', np.mean(Phi_samples))\n",
    "print('StdDev', np.std(Phi_samples))\n",
    "print('RMS', np.mean(4 * Phi_samples**2))\n",
    "\n",
    "print('Mean', np.mean(phase_samples))\n",
    "print('StdDev', np.std(phase_samples))\n",
    "print('RMS', np.mean(4 * phase_samples**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite fast, maybe for this round of pre-training I should be generating data on the fly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing 2D Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nGenerating {num_samples} samples...')\n",
    "Phi_samples = np.zeros((num_samples, Phi_dim, Phi_dim, Phi_dim, Phi_dim))\n",
    "phase_samples = np.zeros((num_samples, num_pix, num_pix))\n",
    "for i in tqdm(range(num_samples)):\n",
    "    # Generate random phase\n",
    "    phase = np.random.uniform(-np.pi, np.pi, (num_pix, num_pix))\n",
    "    phase[0, num_pix // 2 + 1] = 0\n",
    "    phase_samples[i, :, :] = phase\n",
    "\n",
    "    # Compute Phi matrix\n",
    "    Phi = Fluorescence2D.compute_Phi_from_phase(phase)\n",
    "    Phi_samples[i, :, :, :, :] = Phi\n",
    "print(Phi_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing 1D Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nGenerating {num_samples} samples...')\n",
    "Phi_samples = np.zeros((num_samples, Phi_dim, Phi_dim))\n",
    "phase_samples = np.zeros((num_samples, num_pix))\n",
    "fluo = Fluorescence1D(kmax=3, num_pix=num_pix, num_atoms=4)\n",
    "for i in tqdm(range(num_samples)):\n",
    "    fluo.num_atoms = np.random.randint(3, 20)\n",
    "    fluo.randomize_coords()\n",
    "    phase_samples[i, :] = fluo.coh_phase\n",
    "    Phi_samples[i, :, :] = np.arccos(\n",
    "        fluo.cosPhi_from_phase()\n",
    "    )  # returned value here is absPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean', np.mean(Phi_samples))\n",
    "print('StdDev', np.std(Phi_samples))\n",
    "print('RMS', np.mean(4 * Phi_samples**2))\n",
    "\n",
    "print('Mean', np.mean(phase_samples))\n",
    "print('StdDev', np.std(phase_samples))\n",
    "print('RMS', np.mean(4 * phase_samples**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is slower, because we are doing the same computation for the random phases above but adding a process that computes particular phases. If we wanted to generate inputs on-the-fly to save memory, we could just compute a bunch of these \"special\" phases and perform the last (fast) step to compute Phi in the DataLoader.\n",
    "\n",
    "Additionally, we could probably make randomize_coords a little bit more efficient and check cProfile results. Probably not a lot to gain though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_samples)):\n",
    "    phase = phase_samples[i, :]\n",
    "    # Compute Phi matrix\n",
    "    Phi = Fluorescence1D.compute_Phi_from_phase(phase[num_pix - 1 :])\n",
    "    Phi_samples[i, :, :] = Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we pre-computed the phases in this previous loop, we achieve the best iteration speed yet of nearly 134e3 it/s. So perhaps even in the case where we are sampling random phases and we want to perform on-the-fly computation of the inputs to reach very large model sizes in 2D, it would be beneficial to pre-compute the random phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing 2D Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nGenerating {num_samples} samples...')\n",
    "Phi_samples = np.zeros((num_samples, Phi_dim, Phi_dim, Phi_dim, Phi_dim))\n",
    "phase_samples = np.zeros((num_samples, num_pix, num_pix))\n",
    "fluo = Fluorescence2D(kmax=3, num_pix=num_pix, num_atoms=4)\n",
    "for i in tqdm(range(num_samples)):\n",
    "    fluo.num_atoms = np.random.randint(3, 20)\n",
    "    fluo.randomize_coords()\n",
    "    phase_samples[i, :, :] = fluo.coh_phase\n",
    "    Phi_samples[i, :, :, :, :] = np.arccos(\n",
    "        fluo.cosPhi_from_phase()\n",
    "    )  # returned value here is absPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_samples)):\n",
    "    phase = phase_samples[i, :, :]\n",
    "    # Compute Phi matrix\n",
    "    Phi = Fluorescence2D.compute_Phi_from_phase(phase[num_pix - 1 :, num_pix - 1 :])\n",
    "    Phi_samples[i, :, :, :, :] = Phi\n",
    "print(Phi_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $g^{(3)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shots = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluo = Fluorescence1D(kmax=3, num_pix=num_pix, num_atoms=4)\n",
    "for i in tqdm(range(num_samples)):\n",
    "    fluo.num_atoms = np.random.randint(3, 20)\n",
    "    fluo.randomize_coords()\n",
    "    g3 = fluo.marginalize_g3(num_shots=num_shots)\n",
    "    # We only need a small slice of this, can we avoid generating the whole 2*num_pix by 2*num_pix array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are memory limited in how many g3 we can generate (if we have to pass through the 3D/6D array) or store (large datasets going as detector area squared), how efficient is it to generate this final round of fine-tuning data on-the-fly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $g^{(3)}$ Discretization Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is encoding when kmax is large and num_pix is small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
